Wed May 31 05:02:37 UTC 2023
2048
2023-05-benchmarks/default_govverner/2dimm
65B
ggml-model-q4_0.bin
------------------- cpurange ---------------------
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 1 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509357
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time = 22252.36 ms
llama_print_timings:      sample time =    20.27 ms /    40 runs   (    0.51 ms per token)
llama_print_timings: prompt eval time =  6597.40 ms /     2 tokens ( 3298.70 ms per token)
llama_print_timings:        eval time = 101337.81 ms /    39 runs   ( 2598.41 ms per token)
llama_print_timings:       total time = 123673.04 ms
------------------- RUN ---------------------
COMMAND: ./main -t 1 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509481
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  5608.41 ms
llama_print_timings:      sample time =    12.07 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =  4936.65 ms /     2 tokens ( 2468.33 ms per token)
llama_print_timings:        eval time = 119401.74 ms /    39 runs   ( 3061.58 ms per token)
llama_print_timings:       total time = 125027.30 ms
------------------- RUN ---------------------
COMMAND: ./main -t 1 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509607
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  5522.40 ms
llama_print_timings:      sample time =    11.94 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =  5014.80 ms /     2 tokens ( 2507.40 ms per token)
llama_print_timings:        eval time = 100431.71 ms /    39 runs   ( 2575.17 ms per token)
llama_print_timings:       total time = 105971.16 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 2 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509713
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 2 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  3248.82 ms
llama_print_timings:      sample time =    12.47 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  2741.34 ms /     2 tokens ( 1370.67 ms per token)
llama_print_timings:        eval time = 54316.92 ms /    39 runs   ( 1392.74 ms per token)
llama_print_timings:       total time = 57583.59 ms
------------------- RUN ---------------------
COMMAND: ./main -t 2 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509771
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 2 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  3245.48 ms
llama_print_timings:      sample time =    12.72 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  2739.30 ms /     2 tokens ( 1369.65 ms per token)
llama_print_timings:        eval time = 54604.89 ms /    39 runs   ( 1400.13 ms per token)
llama_print_timings:       total time = 57868.37 ms
------------------- RUN ---------------------
COMMAND: ./main -t 2 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509829
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 2 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  3255.93 ms
llama_print_timings:      sample time =    12.42 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  2750.13 ms /     2 tokens ( 1375.07 ms per token)
llama_print_timings:        eval time = 55196.40 ms /    39 runs   ( 1415.29 ms per token)
llama_print_timings:       total time = 58470.02 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 3 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509888
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 3 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  2403.64 ms
llama_print_timings:      sample time =    12.99 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1905.75 ms /     2 tokens (  952.87 ms per token)
llama_print_timings:        eval time = 38063.98 ms /    39 runs   (  976.00 ms per token)
llama_print_timings:       total time = 40485.89 ms
------------------- RUN ---------------------
COMMAND: ./main -t 3 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509929
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 3 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  2382.23 ms
llama_print_timings:      sample time =    12.41 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1882.18 ms /     2 tokens (  941.09 ms per token)
llama_print_timings:        eval time = 37996.19 ms /    39 runs   (  974.26 ms per token)
llama_print_timings:       total time = 40396.15 ms
------------------- RUN ---------------------
COMMAND: ./main -t 3 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685509970
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 3 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  2396.61 ms
llama_print_timings:      sample time =    12.78 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1898.04 ms /     2 tokens (  949.02 ms per token)
llama_print_timings:        eval time = 38270.17 ms /    39 runs   (  981.29 ms per token)
llama_print_timings:       total time = 40684.80 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 4 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510011
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 4 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  2001.58 ms
llama_print_timings:      sample time =    12.55 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1472.45 ms /     2 tokens (  736.23 ms per token)
llama_print_timings:        eval time = 30190.36 ms /    39 runs   (  774.11 ms per token)
llama_print_timings:       total time = 32209.83 ms
------------------- RUN ---------------------
COMMAND: ./main -t 4 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510043
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 4 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1967.01 ms
llama_print_timings:      sample time =    12.57 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1463.66 ms /     2 tokens (  731.83 ms per token)
llama_print_timings:        eval time = 34120.77 ms /    39 runs   (  874.89 ms per token)
llama_print_timings:       total time = 36105.71 ms
------------------- RUN ---------------------
COMMAND: ./main -t 4 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510080
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 4 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1975.13 ms
llama_print_timings:      sample time =    12.68 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1472.13 ms /     2 tokens (  736.07 ms per token)
llama_print_timings:        eval time = 30208.40 ms /    39 runs   (  774.57 ms per token)
llama_print_timings:       total time = 32201.56 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 5 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510112
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 5 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1710.58 ms
llama_print_timings:      sample time =    12.60 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1211.14 ms /     2 tokens (  605.57 ms per token)
llama_print_timings:        eval time = 26736.97 ms /    39 runs   (  685.56 ms per token)
llama_print_timings:       total time = 28465.45 ms
------------------- RUN ---------------------
COMMAND: ./main -t 5 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510141
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 5 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1710.15 ms
llama_print_timings:      sample time =    12.77 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1208.84 ms /     2 tokens (  604.42 ms per token)
llama_print_timings:        eval time = 26882.36 ms /    39 runs   (  689.29 ms per token)
llama_print_timings:       total time = 28610.56 ms
------------------- RUN ---------------------
COMMAND: ./main -t 5 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510170
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 5 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1711.97 ms
llama_print_timings:      sample time =    12.80 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1211.61 ms /     2 tokens (  605.81 ms per token)
llama_print_timings:        eval time = 26601.58 ms /    39 runs   (  682.09 ms per token)
llama_print_timings:       total time = 28331.67 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 6 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510198
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 6 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1568.56 ms
llama_print_timings:      sample time =    12.43 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1058.20 ms /     2 tokens (  529.10 ms per token)
llama_print_timings:        eval time = 25291.38 ms /    39 runs   (  648.50 ms per token)
llama_print_timings:       total time = 26877.69 ms
------------------- RUN ---------------------
COMMAND: ./main -t 6 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510226
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 6 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1560.44 ms
llama_print_timings:      sample time =    12.65 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =  1052.65 ms /     2 tokens (  526.32 ms per token)
llama_print_timings:        eval time = 25234.22 ms /    39 runs   (  647.03 ms per token)
llama_print_timings:       total time = 26812.71 ms
------------------- RUN ---------------------
COMMAND: ./main -t 6 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510253
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 6 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1571.25 ms
llama_print_timings:      sample time =    12.29 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =  1065.69 ms /     2 tokens (  532.85 ms per token)
llama_print_timings:        eval time = 25245.64 ms /    39 runs   (  647.32 ms per token)
llama_print_timings:       total time = 26834.47 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 7 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510280
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 7 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1464.80 ms
llama_print_timings:      sample time =    12.67 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   957.95 ms /     2 tokens (  478.97 ms per token)
llama_print_timings:        eval time = 25069.37 ms /    39 runs   (  642.80 ms per token)
llama_print_timings:       total time = 26552.19 ms
------------------- RUN ---------------------
COMMAND: ./main -t 7 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510307
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 7 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1485.43 ms
llama_print_timings:      sample time =    12.44 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   979.52 ms /     2 tokens (  489.76 ms per token)
llama_print_timings:        eval time = 25076.56 ms /    39 runs   (  642.99 ms per token)
llama_print_timings:       total time = 26579.78 ms
------------------- RUN ---------------------
COMMAND: ./main -t 7 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510334
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 7 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1462.72 ms
llama_print_timings:      sample time =    12.69 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   954.81 ms /     2 tokens (  477.40 ms per token)
llama_print_timings:        eval time = 24887.68 ms /    39 runs   (  638.15 ms per token)
llama_print_timings:       total time = 26368.41 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 8 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510361
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 8 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1392.46 ms
llama_print_timings:      sample time =    12.79 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   888.16 ms /     2 tokens (  444.08 ms per token)
llama_print_timings:        eval time = 24728.40 ms /    39 runs   (  634.06 ms per token)
llama_print_timings:       total time = 26138.93 ms
------------------- RUN ---------------------
COMMAND: ./main -t 8 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510387
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 8 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1395.63 ms
llama_print_timings:      sample time =    12.80 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   889.45 ms /     2 tokens (  444.72 ms per token)
llama_print_timings:        eval time = 24661.17 ms /    39 runs   (  632.34 ms per token)
llama_print_timings:       total time = 26074.95 ms
------------------- RUN ---------------------
COMMAND: ./main -t 8 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510413
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 8 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1384.36 ms
llama_print_timings:      sample time =    12.62 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   877.90 ms /     2 tokens (  438.95 ms per token)
llama_print_timings:        eval time = 24615.69 ms /    39 runs   (  631.17 ms per token)
llama_print_timings:       total time = 26017.94 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 9 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510440
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 9 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1337.96 ms
llama_print_timings:      sample time =    12.17 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =   829.47 ms /     2 tokens (  414.74 ms per token)
llama_print_timings:        eval time = 25048.21 ms /    39 runs   (  642.26 ms per token)
llama_print_timings:       total time = 26403.66 ms
------------------- RUN ---------------------
COMMAND: ./main -t 9 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510467
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 9 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1343.96 ms
llama_print_timings:      sample time =    12.77 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   836.86 ms /     2 tokens (  418.43 ms per token)
llama_print_timings:        eval time = 24979.16 ms /    39 runs   (  640.49 ms per token)
llama_print_timings:       total time = 26341.23 ms
------------------- RUN ---------------------
COMMAND: ./main -t 9 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510493
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 9 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1324.37 ms
llama_print_timings:      sample time =    12.71 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   817.53 ms /     2 tokens (  408.76 ms per token)
llama_print_timings:        eval time = 25044.15 ms /    39 runs   (  642.16 ms per token)
llama_print_timings:       total time = 26386.59 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 10 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510520
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 10 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1315.27 ms
llama_print_timings:      sample time =    12.69 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   802.06 ms /     2 tokens (  401.03 ms per token)
llama_print_timings:        eval time = 24847.42 ms /    39 runs   (  637.11 ms per token)
llama_print_timings:       total time = 26180.70 ms
------------------- RUN ---------------------
COMMAND: ./main -t 10 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510546
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 10 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1309.71 ms
llama_print_timings:      sample time =    12.04 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =   805.57 ms /     2 tokens (  402.79 ms per token)
llama_print_timings:        eval time = 24644.38 ms /    39 runs   (  631.91 ms per token)
llama_print_timings:       total time = 25971.43 ms
------------------- RUN ---------------------
COMMAND: ./main -t 10 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510573
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 10 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1304.08 ms
llama_print_timings:      sample time =    12.39 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   798.66 ms /     2 tokens (  399.33 ms per token)
llama_print_timings:        eval time = 24766.35 ms /    39 runs   (  635.03 ms per token)
llama_print_timings:       total time = 26088.12 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 11 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510599
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 11 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1296.88 ms
llama_print_timings:      sample time =    12.49 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   791.67 ms /     2 tokens (  395.84 ms per token)
llama_print_timings:        eval time = 25175.97 ms /    39 runs   (  645.54 ms per token)
llama_print_timings:       total time = 26490.66 ms
------------------- RUN ---------------------
COMMAND: ./main -t 11 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510626
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 11 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1292.04 ms
llama_print_timings:      sample time =    12.76 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   784.99 ms /     2 tokens (  392.50 ms per token)
llama_print_timings:        eval time = 25184.01 ms /    39 runs   (  645.74 ms per token)
llama_print_timings:       total time = 26494.14 ms
------------------- RUN ---------------------
COMMAND: ./main -t 11 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510653
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 11 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1302.24 ms
llama_print_timings:      sample time =    12.76 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   794.13 ms /     2 tokens (  397.07 ms per token)
llama_print_timings:        eval time = 25173.05 ms /    39 runs   (  645.46 ms per token)
llama_print_timings:       total time = 26493.36 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 12 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510680
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 12 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1297.01 ms
llama_print_timings:      sample time =    12.39 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   786.13 ms /     2 tokens (  393.06 ms per token)
llama_print_timings:        eval time = 25130.17 ms /    39 runs   (  644.36 ms per token)
llama_print_timings:       total time = 26444.93 ms
------------------- RUN ---------------------
COMMAND: ./main -t 12 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510706
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 12 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1301.79 ms
llama_print_timings:      sample time =    12.87 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   787.23 ms /     2 tokens (  393.62 ms per token)
llama_print_timings:        eval time = 25085.05 ms /    39 runs   (  643.21 ms per token)
llama_print_timings:       total time = 26405.02 ms
------------------- RUN ---------------------
COMMAND: ./main -t 12 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510733
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 12 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1301.44 ms
llama_print_timings:      sample time =    12.32 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   786.44 ms /     2 tokens (  393.22 ms per token)
llama_print_timings:        eval time = 25283.38 ms /    39 runs   (  648.29 ms per token)
llama_print_timings:       total time = 26602.45 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 13 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510760
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 13 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1303.18 ms
llama_print_timings:      sample time =    12.24 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   793.15 ms /     2 tokens (  396.57 ms per token)
llama_print_timings:        eval time = 25361.04 ms /    39 runs   (  650.28 ms per token)
llama_print_timings:       total time = 26681.80 ms
------------------- RUN ---------------------
COMMAND: ./main -t 13 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510787
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 13 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1302.77 ms
llama_print_timings:      sample time =    11.28 ms /    40 runs   (    0.28 ms per token)
llama_print_timings: prompt eval time =   792.23 ms /     2 tokens (  396.11 ms per token)
llama_print_timings:        eval time = 25371.93 ms /    39 runs   (  650.56 ms per token)
llama_print_timings:       total time = 26691.31 ms
------------------- RUN ---------------------
COMMAND: ./main -t 13 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510814
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 13 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1285.96 ms
llama_print_timings:      sample time =    12.35 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   781.78 ms /     2 tokens (  390.89 ms per token)
llama_print_timings:        eval time = 25202.66 ms /    39 runs   (  646.22 ms per token)
llama_print_timings:       total time = 26506.32 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 14 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510841
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 14 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1295.48 ms
llama_print_timings:      sample time =    12.85 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   788.65 ms /     2 tokens (  394.33 ms per token)
llama_print_timings:        eval time = 25480.24 ms /    39 runs   (  653.34 ms per token)
llama_print_timings:       total time = 26793.94 ms
------------------- RUN ---------------------
COMMAND: ./main -t 14 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510868
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 14 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1299.60 ms
llama_print_timings:      sample time =    12.88 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   790.25 ms /     2 tokens (  395.13 ms per token)
llama_print_timings:        eval time = 25445.70 ms /    39 runs   (  652.45 ms per token)
llama_print_timings:       total time = 26763.56 ms
------------------- RUN ---------------------
COMMAND: ./main -t 14 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510895
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 14 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1297.40 ms
llama_print_timings:      sample time =    12.47 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   787.99 ms /     2 tokens (  393.99 ms per token)
llama_print_timings:        eval time = 25464.20 ms /    39 runs   (  652.93 ms per token)
llama_print_timings:       total time = 26779.45 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 15 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510922
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 15 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1301.71 ms
llama_print_timings:      sample time =    12.48 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   795.09 ms /     2 tokens (  397.54 ms per token)
llama_print_timings:        eval time = 25588.65 ms /    39 runs   (  656.12 ms per token)
llama_print_timings:       total time = 26908.26 ms
------------------- RUN ---------------------
COMMAND: ./main -t 15 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510950
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 15 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1293.79 ms
llama_print_timings:      sample time =    13.06 ms /    40 runs   (    0.33 ms per token)
llama_print_timings: prompt eval time =   784.66 ms /     2 tokens (  392.33 ms per token)
llama_print_timings:        eval time = 25339.65 ms /    39 runs   (  649.73 ms per token)
llama_print_timings:       total time = 26651.90 ms
------------------- RUN ---------------------
COMMAND: ./main -t 15 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685510977
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 15 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1299.68 ms
llama_print_timings:      sample time =    12.83 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   792.84 ms /     2 tokens (  396.42 ms per token)
llama_print_timings:        eval time = 25623.34 ms /    39 runs   (  657.01 ms per token)
llama_print_timings:       total time = 26941.31 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 16 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511004
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1294.33 ms
llama_print_timings:      sample time =    13.02 ms /    40 runs   (    0.33 ms per token)
llama_print_timings: prompt eval time =   785.29 ms /     2 tokens (  392.64 ms per token)
llama_print_timings:        eval time = 25549.13 ms /    39 runs   (  655.11 ms per token)
llama_print_timings:       total time = 26861.89 ms
------------------- RUN ---------------------
COMMAND: ./main -t 16 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511031
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1304.89 ms
llama_print_timings:      sample time =    13.08 ms /    40 runs   (    0.33 ms per token)
llama_print_timings: prompt eval time =   794.69 ms /     2 tokens (  397.34 ms per token)
llama_print_timings:        eval time = 25735.00 ms /    39 runs   (  659.87 ms per token)
llama_print_timings:       total time = 27058.45 ms
------------------- RUN ---------------------
COMMAND: ./main -t 16 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511059
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1293.05 ms
llama_print_timings:      sample time =    12.07 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =   790.96 ms /     2 tokens (  395.48 ms per token)
llama_print_timings:        eval time = 25590.48 ms /    39 runs   (  656.17 ms per token)
llama_print_timings:       total time = 26901.06 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 17 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511086
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 17 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0


 [end of text]

llama_print_timings:        load time =  1501.56 ms
llama_print_timings:      sample time =     9.39 ms /    31 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =   991.87 ms /     2 tokens (  495.94 ms per token)
llama_print_timings:        eval time = 22136.86 ms /    30 runs   (  737.90 ms per token)
llama_print_timings:       total time = 23652.06 ms
------------------- RUN ---------------------
COMMAND: ./main -t 17 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511110
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 17 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1521.42 ms
llama_print_timings:      sample time =    11.94 ms /    40 runs   (    0.30 ms per token)
llama_print_timings: prompt eval time =  1013.60 ms /     2 tokens (  506.80 ms per token)
llama_print_timings:        eval time = 29407.04 ms /    39 runs   (  754.03 ms per token)
llama_print_timings:       total time = 30945.80 ms
------------------- RUN ---------------------
COMMAND: ./main -t 17 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511141
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 17 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1502.08 ms
llama_print_timings:      sample time =    12.83 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   992.79 ms /     2 tokens (  496.39 ms per token)
llama_print_timings:        eval time = 28763.50 ms /    39 runs   (  737.53 ms per token)
llama_print_timings:       total time = 30283.85 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 18 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511172
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 18 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1487.70 ms
llama_print_timings:      sample time =    11.63 ms /    40 runs   (    0.29 ms per token)
llama_print_timings: prompt eval time =   976.94 ms /     2 tokens (  488.47 ms per token)
llama_print_timings:        eval time = 28248.59 ms /    39 runs   (  724.32 ms per token)
llama_print_timings:       total time = 29753.35 ms
------------------- RUN ---------------------
COMMAND: ./main -t 18 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511202
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 18 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1469.09 ms
llama_print_timings:      sample time =    12.62 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   961.09 ms /     2 tokens (  480.55 ms per token)
llama_print_timings:        eval time = 27861.42 ms /    39 runs   (  714.40 ms per token)
llama_print_timings:       total time = 29348.56 ms
------------------- RUN ---------------------
COMMAND: ./main -t 18 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511231
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 18 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1478.46 ms
llama_print_timings:      sample time =    12.29 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   972.20 ms /     2 tokens (  486.10 ms per token)
llama_print_timings:        eval time = 28360.16 ms /    39 runs   (  727.18 ms per token)
llama_print_timings:       total time = 29856.33 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 19 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511262
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 19 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1429.21 ms
llama_print_timings:      sample time =    12.80 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   921.79 ms /     2 tokens (  460.89 ms per token)
llama_print_timings:        eval time = 26894.20 ms /    39 runs   (  689.59 ms per token)
llama_print_timings:       total time = 28341.63 ms
------------------- RUN ---------------------
COMMAND: ./main -t 19 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511290
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 19 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1433.01 ms
llama_print_timings:      sample time =    12.37 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   921.90 ms /     2 tokens (  460.95 ms per token)
llama_print_timings:        eval time = 26881.03 ms /    39 runs   (  689.26 ms per token)
llama_print_timings:       total time = 28331.83 ms
------------------- RUN ---------------------
COMMAND: ./main -t 19 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511319
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 19 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1431.82 ms
llama_print_timings:      sample time =    12.38 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   923.88 ms /     2 tokens (  461.94 ms per token)
llama_print_timings:        eval time = 26886.88 ms /    39 runs   (  689.41 ms per token)
llama_print_timings:       total time = 28336.52 ms
------------------- 3RUN ---------------------
------------------- RUN ---------------------
COMMAND: ./main -t 20 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511348
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 20 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1397.12 ms
llama_print_timings:      sample time =    12.31 ms /    40 runs   (    0.31 ms per token)
llama_print_timings: prompt eval time =   890.49 ms /     2 tokens (  445.25 ms per token)
llama_print_timings:        eval time = 26152.27 ms /    39 runs   (  670.57 ms per token)
llama_print_timings:       total time = 27567.17 ms
------------------- RUN ---------------------
COMMAND: ./main -t 20 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511376
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 20 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1399.14 ms
llama_print_timings:      sample time =    13.43 ms /    40 runs   (    0.34 ms per token)
llama_print_timings: prompt eval time =   890.07 ms /     2 tokens (  445.04 ms per token)
llama_print_timings:        eval time = 26164.91 ms /    39 runs   (  670.90 ms per token)
llama_print_timings:       total time = 27582.92 ms
------------------- RUN ---------------------
COMMAND: ./main -t 20 -m ./models/65B/ggml-model-q4_0.bin -n 40 --ctx-size 2048
main: build = 588 (ac7876a)
main: seed  = 1685511404
llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 22016
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 65B
llama_model_load_internal: ggml ctx size =    0.18 MB
llama_model_load_internal: mem required  = 38610.46 MB (+ 5120.00 MB per state)
.
llama_init_from_file: kv self size  = 5120.00 MB

system_info: n_threads = 20 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 2048, n_batch = 512, n_predict = 40, n_keep = 0



llama_print_timings:        load time =  1413.87 ms
llama_print_timings:      sample time =    12.82 ms /    40 runs   (    0.32 ms per token)
llama_print_timings: prompt eval time =   904.97 ms /     2 tokens (  452.48 ms per token)
llama_print_timings:        eval time = 26723.86 ms /    39 runs   (  685.23 ms per token)
llama_print_timings:       total time = 28155.98 ms
